<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DataCenter GPU Resources & Compute Pairing</title>
    <link rel="stylesheet" href="../../assets/css/styles.css">
    <link rel="stylesheet" href="../../assets/css/demo-enhancements.css">
    <style>
        .gpu-tier { background: linear-gradient(135deg, #10b981 0%, #059669 100%); }
        .compute-pairing { background: linear-gradient(135deg, #3b82f6 0%, #1d4ed8 100%); }
        .gpu-component { border-left: 4px solid #10b981; background: #f0fdf4; }
        .architecture-layer { background: #eff6ff; border: 2px solid #3b82f6; }
        .gpu-demo { background: #f8fafc; border: 1px solid #e2e8f0; }
        .performance-metric { background: #fefce8; border-left: 3px solid #eab308; }
        .fabric-topology { background: #f3e8ff; border: 2px solid #8b5cf6; }
    </style>
</head>

<body>
    <header class="demo-header">
        <nav>
            <a href="../../index.html">üè† Course Home</a>
            <a href="../module-0-crypto/index.html">üìñ Module 0</a>
            <a href="index.html">üîß Module 1</a>
        </nav>
        <h1>üöÄ DataCenter GPU Resources & Compute Architecture</h1>
        <p>Hardware Design, Resource Pairing & Management</p>
    </header>

    <main class="demo-container">
        <!-- GPU Architecture Overview -->
        <section class="demo-section">
            <h2 class="section-title gpu-tier">üéØ GPU Architecture & Types</h2>

            <div class="gpu-component">
                <h3>üèóÔ∏è GPU Categories in DataCenters</h3>

                <div class="gpu-demo">
                    <h4>AI/ML Training GPUs (Flagship Performance)</h4>
                    <ul>
                        <li><strong>NVIDIA H100 SXM:</strong> 80GB HBM3, 3.35 TB/s memory bandwidth, 989 TF BF16</li>
                        <li><strong>NVIDIA A100 SXM:</strong> 80GB HBM2e, 2.0 TB/s memory bandwidth, 624 TF BF16</li>
                        <li><strong>AMD MI250X:</strong> 128GB HBM2e, 3.2 TB/s memory bandwidth, 383 TF BF16</li>
                        <li><strong>Intel Ponte Vecchio:</strong> 128GB HBM2e, 2.6 TB/s memory bandwidth, 838 TF BF16</li>
                        <li><strong>Use Cases:</strong> Large language models, computer vision, deep learning research</li>
                        <li><strong>Form Factor:</strong> SXM modules in 8-GPU baseboard configurations</li>
                    </ul>
                </div>

                <div class="gpu-demo">
                    <h4>AI/ML Inference GPUs (Optimized Efficiency)</h4>
                    <ul>
                        <li><strong>NVIDIA L40S:</strong> 48GB GDDR6, 864 GB/s bandwidth, 733 TF BF16, AV1 encode/decode</li>
                        <li><strong>NVIDIA L4:</strong> 24GB GDDR6, 300 GB/s bandwidth, 242 TF BF16, video acceleration</li>
                        <li><strong>AMD MI210:</strong> 64GB HBM2e, 1.6 TB/s bandwidth, 181 TF BF16</li>
                        <li><strong>Use Cases:</strong> Real-time inference, edge AI, video processing, recommendation systems</li>
                        <li><strong>Deployment:</strong> PCIe cards, higher density per rack unit</li>
                    </ul>
                </div>

                <div class="gpu-demo">
                    <h4>Multi-Instance GPUs (MIG) & Virtualization</h4>
                    <ul>
                        <li><strong>NVIDIA A100 MIG:</strong> Up to 7 instances per GPU (1g.10gb, 2g.20gb, 3g.40gb, 7g.80gb)</li>
                        <li><strong>NVIDIA H100 MIG:</strong> Up to 7 instances with enhanced isolation</li>
                        <li><strong>GPU Partitioning:</strong> Dedicated SM units, memory slices, cache isolation</li>
                        <li><strong>Use Cases:</strong> Multi-tenant inference, development environments, CI/CD pipelines</li>
                        <li><strong>Management:</strong> Kubernetes device plugins, NUMA topology awareness</li>
                    </ul>
                </div>
            </div>
        </section>

        <!-- Compute Pairing Architecture -->
        <section class="demo-section">
            <h2 class="section-title compute-pairing">üîó Compute-GPU Pairing Architecture</h2>

            <div class="architecture-layer">
                <h3>üíª CPU-GPU Pairing Strategies</h3>

                <div class="gpu-demo">
                    <h4>Balanced Pairing (General Workloads)</h4>
                    <ul>
                        <li><strong>CPU Configuration:</strong> 2x Intel Xeon 8480+ (56 cores) or AMD EPYC 9654 (96 cores)</li>
                        <li><strong>GPU Allocation:</strong> 4-8x NVIDIA L40S or A100 per dual-socket node</li>
                        <li><strong>Memory Ratio:</strong> 512GB-1TB system RAM : 48-80GB GPU memory per card</li>
                        <li><strong>PCIe Configuration:</strong> PCIe 5.0 x16 per GPU, NUMA-aware placement</li>
                        <li><strong>Use Cases:</strong> Mixed workloads, multi-user environments, development clusters</li>
                    </ul>
                </div>

                <div class="gpu-demo">
                    <h4>Compute-Dense Pairing (Training Workloads)</h4>
                    <ul>
                        <li><strong>CPU Configuration:</strong> 2x Intel Xeon 8490H (60 cores) or AMD EPYC 9684X (96 cores)</li>
                        <li><strong>GPU Allocation:</strong> 8x NVIDIA H100 SXM in NVLink domain</li>
                        <li><strong>Memory Configuration:</strong> 2TB system RAM, 640GB total GPU memory</li>
                        <li><strong>Interconnect:</strong> NVLink 4.0 at 900GB/s bidirectional between GPUs</li>
                        <li><strong>Use Cases:</strong> Large model training, distributed computing, HPC simulations</li>
                    </ul>
                </div>

                <div class="gpu-demo">
                    <h4>Inference-Optimized Pairing (Production Serving)</h4>
                    <ul>
                        <li><strong>CPU Configuration:</strong> Higher core count for request processing (2x AMD EPYC 9754)</li>
                        <li><strong>GPU Allocation:</strong> 16x NVIDIA L4 in 4U server for maximum density</li>
                        <li><strong>Memory Strategy:</strong> Lower GPU memory per card, higher system memory for caching</li>
                        <li><strong>Network:</strong> 200GbE+ for low-latency request handling</li>
                        <li><strong>Use Cases:</strong> Real-time inference, API serving, edge deployment</li>
                    </ul>
                </div>
            </div>
        </section>

        <!-- GPU Fabric & Interconnects -->
        <section class="demo-section">
            <h2 class="section-title">üåê GPU Fabric & Interconnect Architecture</h2>

            <div class="fabric-topology">
                <h3>‚ö° High-Speed GPU Interconnects</h3>

                <div class="gpu-demo">
                    <h4>NVIDIA NVLink Architecture</h4>
                    <ul>
                        <li><strong>NVLink 4.0 (H100):</strong> 900GB/s bidirectional between GPUs</li>
                        <li><strong>NVLink 3.0 (A100):</strong> 600GB/s bidirectional between GPUs</li>
                        <li><strong>NVSwitch Architecture:</strong> 64 NVLink ports at 3.6TB/s aggregate bandwidth</li>
                        <li><strong>Multi-Node Scaling:</strong> InfiniBand NDR 400Gb/s for inter-node communication</li>
                        <li><strong>GPU Direct:</strong> Direct memory access between GPUs and network/storage</li>
                    </ul>

                    <h4>Multi-Node GPU Cluster Topology</h4>
                    <pre class="code-block">
# 8-Node H100 Cluster Configuration
Node Layout (per node):
‚îú‚îÄ‚îÄ 8x NVIDIA H100 SXM (80GB each)
‚îú‚îÄ‚îÄ NVSwitch interconnect (all-to-all connectivity)
‚îú‚îÄ‚îÄ 2x InfiniBand NDR400 HCAs (RDMA capable)
‚îî‚îÄ‚îÄ 2x Intel Xeon 8490H CPUs

Inter-Node Fabric:
‚îú‚îÄ‚îÄ InfiniBand Fat-Tree Topology
‚îú‚îÄ‚îÄ 3:1 oversubscription ratio
‚îú‚îÄ‚îÄ RDMA over Converged Ethernet (RoCE) alternative
‚îî‚îÄ‚îÄ GPUDirect RDMA for zero-copy transfers

Total Cluster Specs:
‚îú‚îÄ‚îÄ 64 H100 GPUs (5,120GB total GPU memory)
‚îú‚îÄ‚îÄ 63.4 PetaFLOPS BF16 performance
‚îú‚îÄ‚îÄ 230TB/s intra-node bandwidth
‚îî‚îÄ‚îÄ 25.6TB/s inter-node bandwidth
                    </pre>
                </div>

                <div class="gpu-demo">
                    <h4>AMD GPU Interconnect (xGMI/Infinity Fabric)</h4>
                    <ul>
                        <li><strong>xGMI Links:</strong> 92GB/s per link, up to 8 links per GPU</li>
                        <li><strong>Infinity Fabric:</strong> Coherent memory access across CPU-GPU domain</li>
                        <li><strong>Network Integration:</strong> ROCm integration with InfiniBand and Ethernet</li>
                        <li><strong>Multi-GPU Scaling:</strong> Up to 8x MI250X or 4x MI300X per node</li>
                    </ul>
                </div>

                <div class="gpu-demo">
                    <h4>Intel GPU Fabric (Xe-Link)</h4>
                    <ul>
                        <li><strong>Xe-Link:</strong> 128GB/s bidirectional between Ponte Vecchio GPUs</li>
                        <li><strong>Mesh Topology:</strong> Direct connections in 4-GPU configurations</li>
                        <li><strong>oneAPI Integration:</strong> Unified programming model across CPU-GPU</li>
                    </ul>
                </div>
            </div>
        </section>

        <!-- Resource Management & Orchestration -->
        <section class="demo-section">
            <h2 class="section-title">üõ†Ô∏è GPU Resource Management & Orchestration</h2>

            <div class="architecture-layer">
                <h3>üéõÔ∏è Kubernetes GPU Management</h3>

                <div class="gpu-demo">
                    <h4>GPU Device Plugins Configuration</h4>
                    <pre class="code-block">
# NVIDIA GPU Operator Deployment
apiVersion: v1
kind: Namespace
metadata:
  name: gpu-operator
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nvidia-device-plugin-daemonset
  namespace: gpu-operator
spec:
  selector:
    matchLabels:
      name: nvidia-device-plugin-ds
  template:
    metadata:
      labels:
        name: nvidia-device-plugin-ds
    spec:
      containers:
      - image: nvcr.io/nvidia/k8s-device-plugin:v0.14.1
        name: nvidia-device-plugin-ctr
        env:
        - name: MIG_STRATEGY
          value: "mixed"  # Support both MIG and full GPU allocation
        - name: FAIL_ON_INIT_ERROR
          value: "false"
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
        volumeMounts:
        - name: device-plugin
          mountPath: /var/lib/kubelet/device-plugins
      volumes:
      - name: device-plugin
        hostPath:
          path: /var/lib/kubelet/device-plugins
                    </pre>
                </div>

                <div class="gpu-demo">
                    <h4>Multi-Instance GPU (MIG) Configuration</h4>
                    <pre class="code-block">
# Enable MIG mode on H100 GPUs
nvidia-smi -i 0 -mig 1

# Create MIG instances (example for H100 80GB)
nvidia-smi mig -cgi 1g.10gb,2g.20gb,3g.40gb -C

# Verify MIG configuration
nvidia-smi mig -lgip

# Kubernetes MIG resource requests
apiVersion: v1
kind: Pod
metadata:
  name: training-job-small
spec:
  containers:
  - name: pytorch-training
    image: nvcr.io/nvidia/pytorch:23.08-py3
    resources:
      limits:
        nvidia.com/mig-1g.10gb: 1  # Request 1g.10gb MIG slice
        memory: "16Gi"
        cpu: "8"
      requests:
        nvidia.com/mig-1g.10gb: 1
        memory: "16Gi"
        cpu: "8"
  nodeSelector:
    accelerator: nvidia-h100-mig
                    </pre>
                </div>

                <div class="gpu-demo">
                    <h4>Advanced GPU Scheduling</h4>
                    <pre class="code-block">
# Node affinity for GPU topology awareness
apiVersion: v1
kind: Pod
metadata:
  name: distributed-training
spec:
  containers:
  - name: training-worker
    image: nvcr.io/nvidia/pytorch:23.08-py3
    resources:
      limits:
        nvidia.com/gpu: 8  # Full 8-GPU node
    env:
    - name: NCCL_IB_DISABLE
      value: "0"  # Enable InfiniBand for NCCL
    - name: NCCL_NET_GDR_LEVEL
      value: "PIX"  # Enable GPUDirect RDMA
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: node.kubernetes.io/instance-type
            operator: In
            values: ["gpu-h100-8x"]
          - key: topology.kubernetes.io/zone
            operator: In
            values: ["zone-a"]  # Co-locate for low latency
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchLabels:
              app: distributed-training
          topologyKey: kubernetes.io/hostname
                    </pre>
                </div>
            </div>
        </section>

        <!-- Performance Optimization -->
        <section class="demo-section">
            <h2 class="section-title">‚ö° Performance Optimization & Monitoring</h2>

            <div class="performance-metric">
                <h3>üìä GPU Performance Characteristics</h3>

                <div class="gpu-demo">
                    <h4>Memory Optimization Strategies</h4>
                    <ul>
                        <li><strong>Memory Bandwidth Utilization:</strong> Target >80% of peak bandwidth for compute-bound workloads</li>
                        <li><strong>Tensor Core Optimization:</strong> Use BF16/FP16 data types for 2-4x performance improvement</li>
                        <li><strong>Memory Pooling:</strong> Pre-allocate memory pools to avoid allocation overhead</li>
                        <li><strong>Unified Memory:</strong> Use CUDA Unified Memory for large datasets exceeding GPU memory</li>
                    </ul>

                    <h4>CUDA Performance Analysis</h4>
                    <pre class="code-block">
# GPU utilization monitoring
nvidia-smi dmon -s puct -d 1

# Detailed profiling with Nsight Systems
nsys profile --stats=true python training_script.py

# Memory usage analysis
nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv

# GPU topology information
nvidia-smi topo -m

# NVLink utilization monitoring
nvidia-smi nvlink --status
nvidia-smi nvlink -gt d  # Get NVLink throughput data
                    </pre>
                </div>

                <div class="gpu-demo">
                    <h4>Multi-GPU Communication Optimization</h4>
                    <ul>
                        <li><strong>NCCL Tuning:</strong> Optimize collective operations for distributed training</li>
                        <li><strong>NVLink Utilization:</strong> Maximize intra-node GPU-GPU bandwidth</li>
                        <li><strong>InfiniBand Optimization:</strong> Tune network settings for multi-node scaling</li>
                        <li><strong>Gradient Compression:</strong> Reduce communication overhead in distributed training</li>
                    </ul>

                    <h4>NCCL Performance Tuning</h4>
                    <pre class="code-block">
# NCCL environment variables for optimization
export NCCL_DEBUG=INFO
export NCCL_IB_DISABLE=0           # Enable InfiniBand
export NCCL_NET_GDR_LEVEL=PIX      # Enable GPUDirect RDMA
export NCCL_IB_HCA=mlx5_0,mlx5_1   # Specify InfiniBand adapters
export NCCL_SOCKET_IFNAME=ib0,ib1  # Network interfaces

# Topology-aware communication
export NCCL_TOPO_FILE=/opt/nccl-topology.xml
export NCCL_TREE_THRESHOLD=0       # Force tree algorithm
export NCCL_RING_THRESHOLD=8       # Ring algorithm threshold

# NCCL test for bandwidth measurement
/opt/nccl-tests/build/all_reduce_perf -b 8 -e 1G -f 2 -g 8
                    </pre>
                </div>
            </div>
        </section>

        <!-- Power & Thermal Management -->
        <section class="demo-section">
            <h2 class="section-title">üå°Ô∏è Power & Thermal Management</h2>

            <div class="architecture-layer">
                <h3>‚ö° Power Consumption & Efficiency</h3>

                <div class="gpu-demo">
                    <h4>GPU Power Profiles</h4>
                    <ul>
                        <li><strong>NVIDIA H100 SXM:</strong> 700W TGP, dynamic power scaling based on workload</li>
                        <li><strong>NVIDIA A100 SXM:</strong> 400W TGP, enterprise efficiency optimizations</li>
                        <li><strong>NVIDIA L4:</strong> 72W TGP, inference-optimized power consumption</li>
                        <li><strong>AMD MI250X:</strong> 560W TGP, advanced power management features</li>
                    </ul>

                    <h4>Power Management Commands</h4>
                    <pre class="code-block">
# GPU power monitoring
nvidia-smi --query-gpu=power.draw,power.limit --format=csv -l 1

# Set power limits (requires admin privileges)
nvidia-smi -pl 400  # Set power limit to 400W

# Power efficiency monitoring
nvidia-smi --query-gpu=power.draw,utilization.gpu --format=csv -l 1

# Temperature and throttling monitoring
nvidia-smi --query-gpu=temperature.gpu,temperature.memory,clocks_throttle_reasons.hw_slowdown --format=csv -l 1
                    </pre>
                </div>

                <div class="gpu-demo">
                    <h4>Cooling System Design</h4>
                    <ul>
                        <li><strong>Liquid Cooling:</strong> Direct-to-chip liquid cooling for high-density GPU deployments</li>
                        <li><strong>Airflow Management:</strong> Hot-aisle/cold-aisle configuration with 35-40¬∞C inlet temperature</li>
                        <li><strong>Thermal Throttling:</strong> Automatic frequency reduction at 83¬∞C+ to prevent damage</li>
                        <li><strong>Fan Curves:</strong> Dynamic fan speed control based on GPU temperature and workload</li>
                    </ul>
                </div>
            </div>
        </section>

        <!-- Interactive Demo -->
        <section class="demo-section">
            <h2 class="section-title">üéÆ Interactive GPU Resource Demo</h2>

            <div class="demo-controls">
                <button onclick="demonstrateGPUTopology()" class="demo-button">
                    üó∫Ô∏è GPU Cluster Topology
                </button>
                <button onclick="showResourcePairing()" class="demo-button">
                    üîó Compute-GPU Pairing
                </button>
                <button onclick="simulateWorkloadScaling()" class="demo-button">
                    üìä Workload Scaling
                </button>
                <button onclick="monitorGPUPerformance()" class="demo-button">
                    ‚ö° Performance Monitoring
                </button>
            </div>

            <div id="gpu-demo-output" class="demo-output">
                <p>Click any button above to explore GPU resource management capabilities.</p>
            </div>
        </section>
    </main>

    <footer class="demo-footer">
        <div class="footer-nav">
            <a href="datacenter-nvme-storage.html">‚Üê NVMe Storage</a>
            <a href="threat-modeling-datacenter.html">Threat Modeling ‚Üí</a>
        </div>
        <p>&copy; 2024 DataCenter Hardware Security Course. Educational content for masters-level study.</p>
    </footer>

    <script src="gpu-resources-demo.js"></script>
</body>
</html>